{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Example of LLM (& final project)"
      ],
      "metadata": {
        "id": "WBg7RqzU7qxe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtUjLf727qDw"
      },
      "outputs": [],
      "source": [
        "## Make sure these are all installed and restart the runtime\n",
        "## Also make sure you have a GPU runtime.\n",
        "# pick a small one\n",
        "!pip install accelerate &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### I mounted google drive then made directories for the llm\n",
        "##! mkdir /content/drive/MyDrive/llms\n",
        "##! mdkir /content/drive/MyDrive/llms/MaziyarPanahiT\n",
        "##! mdkir /content/drive/MyDrive/llms/MaziyarPanahi/Llama-3-8B-Instruct-v0.4\n",
        "\n",
        "### Change into the llm root directory\n",
        "%cd /content/drive/MyDrive/llms\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/llms/MaziyarPanahi/Llama-3-8B-Instruct-v0.4\"\n",
        "###\n",
        "### This downloads an entire llm commenting out since I already did it once\n",
        "###\n",
        "#from huggingface_hub import snapshot_download\n",
        "#snapshot_download(repo_id=\"MaziyarPanahi/Llama-3-8B-Instruct-v0.4\", local_dir = \"MaziyarPanahi/Llama-3-8B-Instruct-v0.4/\")\n",
        "\n",
        "#*** for homework, download small model into root directory on Colab, don't need to do all these steps;\n",
        "#**he put this here because he downloaded a big model and doesn't want to download it over and over ***"
      ],
      "metadata": {
        "id": "UUKK4f1E8S4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### First we need to import the collection of tools that allow\n",
        "### us to work with the llm. The transformers package has the basic\n",
        "### utility of working with llms whereas torch reference pytorch,\n",
        "### the nn framework put out originally by Meta.\n",
        "\n",
        "##**this is for this specific model from huggingface website\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "### Next let's make the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    ## First load the pretrained model, recall we set the path to the model earlier\n",
        "    model_path,\n",
        "    ##Below we set defaults of the library useage\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    # attn_implementation=\"flash_attention_2\"\n",
        ")\n",
        "\n",
        "## The tokenizers takes characters/words/word sets and turns them\n",
        "## into numbers\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_path,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "## The streamer passes text to and from the tokenizer\n",
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "## The pipline now builds the io we'll use\n",
        "my_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "    streamer=streamer\n",
        ")"
      ],
      "metadata": {
        "id": "Hp4ZHfNo9up6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then you can use my_pipeline to generate text.\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = my_pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=512,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.95,\n",
        ")\n",
        "\n",
        "print(outputs[0][\"generated_text\"][len(prompt):])"
      ],
      "metadata": {
        "id": "0YZnf832_Ar4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#searching 3b for a good size"
      ],
      "metadata": {
        "id": "oTB81uVQCOMb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
